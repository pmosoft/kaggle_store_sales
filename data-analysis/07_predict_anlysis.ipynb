{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark \n",
    "findspark.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import *\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "%load_ext sparksql_magic\n",
    "%config SparkSql.limit=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV filcondae I/O (e.g. pd.read_csv)\n",
    "import random as rd # generating random numbers\n",
    "import matplotlib.pyplot as plt # basic plotting\n",
    "import seaborn as sns # for prettier plots\n",
    "import plotly.express as px\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"d:/lge/pycharm-projects/kaggle_store_sales/input\"\n",
    "train_master_all = spark.read.csv(f\"{path}/train_master_all.csv\", inferSchema = True, header = True);train_master_all.createOrReplaceTempView(\"train_master_all\");spark.sql(\"CACHE TABLE train_master_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"work_dtm16\"    , StringType()),\n",
    "    StructField(\"scenario_id\"   , StringType()),\n",
    "    StructField(\"scenario_desc\" , StringType()),\n",
    "    StructField(\"feature_src\"   , StringType()),\n",
    "    StructField(\"feature_col\"   , StringType()),\n",
    "    StructField(\"feature_sdt8\"  , StringType()),\n",
    "    StructField(\"feature_edt8\"  , StringType()),\n",
    "    StructField(\"feature_desc\"  , StringType()),\n",
    "    StructField(\"predict_col\"   , StringType()),\n",
    "    StructField(\"predict_sdt8\"  , StringType()),\n",
    "    StructField(\"predict_edt8\"  , StringType()),\n",
    "    StructField(\"auto_ml_model\" , StringType()),\n",
    "    StructField(\"model_name\"    , StringType()),\n",
    "    StructField(\"model_cfg\"     , StringType()),\n",
    "    StructField(\"store_nbr\"     , StringType()),\n",
    "    StructField(\"family2\"       , StringType()),\n",
    "    StructField(\"test_y\"        , StringType()),\n",
    "    StructField(\"predict_y\"     , StringType()),\n",
    "    StructField(\"mae\"           , StringType()),\n",
    "    StructField(\"mse\"          , StringType()),\n",
    "    StructField(\"rmse\"           , StringType()),\n",
    "    StructField(\"msle\"           , StringType()),\n",
    "    StructField(\"rmsle\"           , StringType()),\n",
    "    StructField(\"r2\"           , StringType()),\n",
    "    StructField(\"score\"         , StringType()),\n",
    "    StructField(\"fit_tm_sec\"    , StringType()),\n",
    "    StructField(\"file_nm\"       , StringType()),\n",
    "    StructField(\"memo\"          , StringType())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"d:/lge/pycharm-projects/kaggle_store_sales/output\"\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "path = \"d:/lge/pycharm-projects/kaggle_store_sales/output/\"\n",
    "files = [path+f for f in listdir(path) if isfile(join(path, f))]\n",
    "results = spark.read.schema(schema).json(files);results.createOrReplaceTempView(\"results\");spark.sql(\"CACHE TABLE results\")\n",
    "#results = spark.read.json(files);results.createOrReplaceTempView(\"results\");spark.sql(\"CACHE TABLE results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario_id</th>\n",
       "      <th>cnt</th>\n",
       "      <th>s90</th>\n",
       "      <th>s70</th>\n",
       "      <th>s50</th>\n",
       "      <th>s30</th>\n",
       "      <th>s10</th>\n",
       "      <th>fit_tm_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x001f004d001y001t001m001c001</td>\n",
       "      <td>1782</td>\n",
       "      <td>906</td>\n",
       "      <td>414</td>\n",
       "      <td>221</td>\n",
       "      <td>101</td>\n",
       "      <td>111</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x001f005d001y001t001m001c001</td>\n",
       "      <td>1782</td>\n",
       "      <td>979</td>\n",
       "      <td>428</td>\n",
       "      <td>179</td>\n",
       "      <td>66</td>\n",
       "      <td>94</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x001f006d001y001t001m001c001</td>\n",
       "      <td>1782</td>\n",
       "      <td>988</td>\n",
       "      <td>419</td>\n",
       "      <td>184</td>\n",
       "      <td>68</td>\n",
       "      <td>92</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x001f007d001y001t001m001c001</td>\n",
       "      <td>1782</td>\n",
       "      <td>983</td>\n",
       "      <td>426</td>\n",
       "      <td>181</td>\n",
       "      <td>66</td>\n",
       "      <td>93</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x001f008d001y001t001m001c001</td>\n",
       "      <td>1782</td>\n",
       "      <td>983</td>\n",
       "      <td>426</td>\n",
       "      <td>181</td>\n",
       "      <td>66</td>\n",
       "      <td>93</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>x001f009d001y001t001m001c001</td>\n",
       "      <td>1782</td>\n",
       "      <td>979</td>\n",
       "      <td>428</td>\n",
       "      <td>179</td>\n",
       "      <td>66</td>\n",
       "      <td>94</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>x001f010d001y001t001m001c001</td>\n",
       "      <td>1782</td>\n",
       "      <td>983</td>\n",
       "      <td>426</td>\n",
       "      <td>181</td>\n",
       "      <td>66</td>\n",
       "      <td>93</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    scenario_id   cnt  s90  s70  s50  s30  s10  fit_tm_min\n",
       "0  x001f004d001y001t001m001c001  1782  906  414  221  101  111         2.0\n",
       "1  x001f005d001y001t001m001c001  1782  979  428  179   66   94         3.0\n",
       "2  x001f006d001y001t001m001c001  1782  988  419  184   68   92         3.0\n",
       "3  x001f007d001y001t001m001c001  1782  983  426  181   66   93         3.0\n",
       "4  x001f008d001y001t001m001c001  1782  983  426  181   66   93         3.0\n",
       "5  x001f009d001y001t001m001c001  1782  979  428  179   66   94         3.0\n",
       "6  x001f010d001y001t001m001c001  1782  983  426  181   66   93         3.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.sql('''\n",
    "select scenario_id\n",
    "     , count(*) cnt\n",
    "     , count(case when score > 0.81 then 1 else null end) as s90       \n",
    "     , count(case when score between 0.61 and 0.80 then 1 else null end) as s70       \n",
    "     , count(case when score between 0.41 and 0.60 then 1 else null end) as s50       \n",
    "     , count(case when score between 0.21 and 0.40 then 1 else null end) as s30      \n",
    "     , count(case when score < 0.20 then 1 else null end) as s10\n",
    "     , round(sum(fit_tm_sec)/60)  as fit_tm_min\n",
    "from results       \n",
    "where 1=1 \n",
    "and scenario_id like'%t00%'\n",
    "and scenario_id like'%t001%'\n",
    "and scenario_id like'%x001%'\n",
    "and scenario_id like'%d001%'\n",
    "and scenario_id like'%m001%'\n",
    "group by scenario_id\n",
    "order by scenario_id\n",
    "''')\n",
    "pdf = df.toPandas()\n",
    "pdf.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">store_nbr</td><td style=\"font-weight: bold\">family2</td><td style=\"font-weight: bold\">scenario_id</td><td style=\"font-weight: bold\">score</td><td style=\"font-weight: bold\">rmsle</td><td style=\"font-weight: bold\">predict_y</td><td style=\"font-weight: bold\">test_y</td><td style=\"font-weight: bold\">r2</td><td style=\"font-weight: bold\">mae</td><td style=\"font-weight: bold\">mse</td><td style=\"font-weight: bold\">rmse</td><td style=\"font-weight: bold\">msle</td></tr><tr><td>1</td><td>1</td><td>x001f004d001y001t001m001c001</td><td>0.69</td><td>null</td><td>[3.7231861938574373,3.5158146469749125,3.308443100092388,3.101071553209864,2.893700006327339,2.6863284594448147,3.878282763134089,3.6709112162515645,3.4635396693690397,3.2561681224865153,3.0487965756039905,2.841425028721466,2.634053481838942,3.826007785528216,3.618636238645691]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>null</td><td>0.0</td><td>0.31</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1</td><td>x002f005d002y001t001m001c001</td><td>0.71</td><td>null</td><td>[5,5,4,4,4,4,5,5,4,4,4,4,4,5,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>null</td><td>0.0</td><td>0.29</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1</td><td>x003f005d002y001t001m001c001</td><td>0.71</td><td>null</td><td>[5,5,4,4,4,4,5,5,4,4,4,4,4,5,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>null</td><td>0.0</td><td>0.29</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1</td><td>x001f004d002y001t001m001c001</td><td>0.71</td><td>null</td><td>[5.18700504810699,5.075747310943138,4.964489573779287,4.853231836615436,4.741974099451585,4.630716362287733,5.196951024635704,5.085693287471852,4.974435550308001,4.86317781314415,4.751920075980298,4.640662338816447,4.529404601652596,5.095639264000566,4.984381526836715]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>null</td><td>0.0</td><td>0.29</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1</td><td>x002f005d001y001t001m003c001</td><td>0.71</td><td>null</td><td>[4,4,4,4,3,3,4,4,4,4,4,3,3,4,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>null</td><td>0.0</td><td>0.29</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1</td><td>x001f005d001y001t001m003c001</td><td>0.71</td><td>null</td><td>[4,4,4,4,3,3,4,4,4,4,4,3,3,4,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>null</td><td>0.0</td><td>0.29</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1</td><td>x003f005d001y001t001m003c001</td><td>0.71</td><td>null</td><td>[4,4,4,4,3,3,4,4,4,4,4,3,3,4,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>null</td><td>0.0</td><td>0.29</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1</td><td>x001f006d002y001t001m001c001</td><td>0.71</td><td>0.54</td><td>[4,4,4,4,4,4,5,4,4,4,4,4,4,5,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>-0.11</td><td>2.07</td><td>6.6</td><td>2.57</td><td>0.29</td></tr><tr><td>1</td><td>1</td><td>x001f005d002y001t001m001c001</td><td>0.71</td><td>0.54</td><td>[5,5,4,4,4,4,5,5,4,4,4,4,4,5,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>-0.12</td><td>2.13</td><td>6.67</td><td>2.58</td><td>0.29</td></tr><tr><td>1</td><td>1</td><td>x001f005d002y001t001m002c001</td><td>0.72</td><td>null</td><td>[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>null</td><td>0.0</td><td>0.28</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1</td><td>x002f005d001y001t001m002c001</td><td>0.72</td><td>null</td><td>[4,4,4,4,4,3,5,4,4,4,4,4,3,5,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>null</td><td>0.0</td><td>0.28</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1</td><td>x002f005d002y001t001m003c001</td><td>0.72</td><td>null</td><td>[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>null</td><td>0.0</td><td>0.28</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1</td><td>x003f005d002y001t001m002c001</td><td>0.72</td><td>null</td><td>[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>null</td><td>0.0</td><td>0.28</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1</td><td>x001f005d001y001t001m002c001</td><td>0.72</td><td>null</td><td>[4,4,4,4,4,3,5,4,4,4,4,4,3,5,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>null</td><td>0.0</td><td>0.28</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1</td><td>x003f005d001y001t001m002c001</td><td>0.72</td><td>null</td><td>[4,4,4,4,4,3,5,4,4,4,4,4,3,5,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>null</td><td>0.0</td><td>0.28</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1</td><td>x003f005d002y001t001m003c001</td><td>0.72</td><td>null</td><td>[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>null</td><td>0.0</td><td>0.28</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1</td><td>x002f005d002y001t001m002c001</td><td>0.72</td><td>null</td><td>[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>null</td><td>0.0</td><td>0.28</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1</td><td>x001f005d002y001t001m003c001</td><td>0.72</td><td>null</td><td>[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>null</td><td>0.0</td><td>0.28</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1</td><td>x001f006d001y001t001m001c001</td><td>0.72</td><td>0.53</td><td>[4,4,4,4,4,3,5,4,4,4,4,4,3,5,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>-0.11</td><td>2.07</td><td>6.6</td><td>2.57</td><td>0.28</td></tr><tr><td>1</td><td>1</td><td>x003f005d001y001t001m001c001</td><td>0.73</td><td>null</td><td>[5,4,4,4,4,4,5,5,4,4,4,4,3,5,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>null</td><td>0.0</td><td>0.27</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1</td><td>x002f005d001y001t001m001c001</td><td>0.73</td><td>null</td><td>[5,4,4,4,4,4,5,5,4,4,4,4,3,5,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>null</td><td>0.0</td><td>0.27</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1</td><td>x001f008d001y001t001m001c001</td><td>0.73</td><td>0.52</td><td>[5,4,4,4,4,4,5,5,4,4,4,4,3,5,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>-0.06</td><td>2.0</td><td>6.27</td><td>2.5</td><td>0.27</td></tr><tr><td>1</td><td>1</td><td>x001f007d001y001t001m001c001</td><td>0.73</td><td>0.52</td><td>[5,4,4,4,4,4,5,5,4,4,4,4,3,5,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>-0.06</td><td>2.0</td><td>6.27</td><td>2.5</td><td>0.27</td></tr><tr><td>1</td><td>1</td><td>x001f010d001y001t001m001c001</td><td>0.73</td><td>0.52</td><td>[5,4,4,4,4,4,5,5,4,4,4,4,3,5,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>-0.06</td><td>2.0</td><td>6.27</td><td>2.5</td><td>0.27</td></tr><tr><td>1</td><td>1</td><td>x001f009d001y001t001m001c001</td><td>0.73</td><td>0.52</td><td>[5,4,4,4,4,4,5,5,4,4,4,4,3,5,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>-0.06</td><td>2.0</td><td>6.27</td><td>2.5</td><td>0.27</td></tr><tr><td>1</td><td>1</td><td>x001f005d001y001t001m001c001</td><td>0.73</td><td>0.52</td><td>[5,4,4,4,4,4,5,5,4,4,4,4,3,5,4]</td><td>[5,4,3,8,5,6,7,4,7,9,1,6,1,1,4]</td><td>-0.06</td><td>2.0</td><td>6.27</td><td>2.5</td><td>0.27</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select store_nbr, family2, scenario_id,score,rmsle,predict_y,test_y,r2, mae,mse,rmse,msle\n",
    "from results       \n",
    "where 1=1 \n",
    "--and scenario_id like'x001f008d001y001t001m001c001'\n",
    "and store_nbr = 1\n",
    "and family2 = 1\n",
    "and scenario_id like'%t001%'\n",
    "--and family2 between 21 and 40\n",
    "--and r2 between 0.1 and 0.3\n",
    "--and rmsle between 0.3 and 0.5\n",
    "order by score, cast(store_nbr as int), cast(family2 as int), rmsle \n",
    "limit 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.sql('''\n",
    "# with temp1 as (\n",
    "#     select store_nbr, family2\n",
    "#          , max(score) as score\n",
    "#          , min(scenario_id) as scenario_id_min\n",
    "#          , max(scenario_id) as scenario_id_max\n",
    "#     from  (select * \n",
    "#            from results\n",
    "#            where  length(predict_y) - length(replace(predict_y,',','')) + 1 > 10\n",
    "#            )\n",
    "#     where  1=1\n",
    "#     and    scenario_id like'%t00%'\n",
    "#     and    scenario_id like'%f005%'\n",
    "#     and    scenario_id = replace(replace(replace(scenario_id,'t001','t002'),'d001','d003'),'d002','d004')\n",
    "#     group by store_nbr, family2\n",
    "#     order by cast(store_nbr as int), cast(family2 as int)\n",
    "# ), temp2 as (\n",
    "#     select a.store_nbr, a.family2, a.scenario_id, a.score, a.predict_y\n",
    "#     from   results a,\n",
    "#            temp1 b\n",
    "#     where  a.store_nbr = b.store_nbr\n",
    "#     and    a.family2   = b.family2\n",
    "#     and    a.score     = b.score\n",
    "#     order by a.store_nbr, a.family2\n",
    "# ), temp3 as (\n",
    "#     select a.store_nbr, a.family2, min(a.scenario_id) as scenario_id, a.score,replace(replace(replace(min(a.scenario_id),'t001','t002'),'d001','d003'),'d002','d004') as scenario_id2\n",
    "#     from   temp2 a\n",
    "#     where  1=1\n",
    "#     --and    store_nbr = 20\n",
    "#     --and    family2 = 1\n",
    "#     group by a.store_nbr, a.family2, a.score\n",
    "#     order by cast(a.store_nbr as int), cast(a.family2 as int), a.score\n",
    "# ), temp4 as (\n",
    "#     select a.store_nbr, a.family2, a.scenario_id, a.score, replace(replace(a.predict_y,'['),']') as predict_y\n",
    "#     from   results a,\n",
    "#            temp3 b\n",
    "#     where  a.store_nbr = b.store_nbr\n",
    "#     and    a.family2   = b.family2\n",
    "#     and    a.scenario_id = b.scenario_id2\n",
    "#     order by cast(a.store_nbr as int), cast(a.family2 as int)\n",
    "# ), temp5 as (\n",
    "#     select  '20170816' as date8, store_nbr, family2 ,split(predict_y,',')[0]  sales, predict_y from temp4 union all\n",
    "#     select  '20170817' as date8, store_nbr, family2 ,split(predict_y,',')[1]  sales, predict_y from temp4 union all\n",
    "#     select  '20170818' as date8, store_nbr, family2 ,split(predict_y,',')[2]  sales, predict_y from temp4 union all\n",
    "#     select  '20170819' as date8, store_nbr, family2 ,split(predict_y,',')[3]  sales, predict_y from temp4 union all\n",
    "#     select  '20170820' as date8, store_nbr, family2 ,split(predict_y,',')[4]  sales, predict_y from temp4 union all\n",
    "#     select  '20170821' as date8, store_nbr, family2 ,split(predict_y,',')[5]  sales, predict_y from temp4 union all\n",
    "#     select  '20170822' as date8, store_nbr, family2 ,split(predict_y,',')[6]  sales, predict_y from temp4 union all\n",
    "#     select  '20170823' as date8, store_nbr, family2 ,split(predict_y,',')[7]  sales, predict_y from temp4 union all\n",
    "#     select  '20170824' as date8, store_nbr, family2 ,split(predict_y,',')[8]  sales, predict_y from temp4 union all\n",
    "#     select  '20170825' as date8, store_nbr, family2 ,split(predict_y,',')[9]  sales, predict_y from temp4 union all\n",
    "#     select  '20170826' as date8, store_nbr, family2 ,split(predict_y,',')[10] sales, predict_y from temp4 union all\n",
    "#     select  '20170827' as date8, store_nbr, family2 ,split(predict_y,',')[11] sales, predict_y from temp4 union all\n",
    "#     select  '20170828' as date8, store_nbr, family2 ,split(predict_y,',')[12] sales, predict_y from temp4 union all\n",
    "#     select  '20170829' as date8, store_nbr, family2 ,split(predict_y,',')[13] sales, predict_y from temp4 union all\n",
    "#     select  '20170830' as date8, store_nbr, family2 ,split(predict_y,',')[14] sales, predict_y from temp4 union all\n",
    "#     select  '20170831' as date8, store_nbr, family2 ,split(predict_y,',')[15] sales, predict_y from temp4\n",
    "# ), temp6 as (\n",
    "#     select  b.id, a.*, b.family\n",
    "#     from    temp5 a, \n",
    "#             train_master_all b\n",
    "#     where   a.store_nbr = b.store_nbr\n",
    "#     and     a.family2   = b.family2\n",
    "#     and     a.date8     = b.date8    \n",
    "# ), temp7 as (\n",
    "#     --select *\n",
    "#     select id, sales\n",
    "#     from   temp6 a\n",
    "#     order by a.id, cast(a.store_nbr as int), cast(a.family2 as int), date8\n",
    "# )\n",
    "# select * from temp7\n",
    "# ''')\n",
    "# #df.show(5,False)\n",
    "# pdf = df.toPandas()\n",
    "# path = \"d:/lge/pycharm-projects/kaggle_store_sales/output\"\n",
    "# pdf.to_csv(f'{path}/submission/submission_cat2007_20220426.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"d:/lge/pycharm-projects/kaggle_store_sales/output\"\n",
    "submission_cat2007_20220426 = spark.read.csv(f\"{path}/submission/submission_cat2007_20220426.csv\", inferSchema = True, header = True);submission_cat2007_20220425.createOrReplaceTempView(\"submission_cat2007_20220426\");spark.sql(\"CACHE TABLE submission_cat2007_20220426\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "\n",
    "select store_nbr, family2, scenario_id, score, predict_y\n",
    "from   results\n",
    "where  1=1\n",
    "--and scenario_id = 'x003f005d003y001t002m003c001'\n",
    "and    store_nbr = 1\n",
    "and    family2 = 1\n",
    "and    scenario_id like'%t002%'\n",
    "limit 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "--select count(*) \n",
    "select *\n",
    "from   submission_cat2007_20220426\n",
    "where sales is null\n",
    "order by id\n",
    "limit 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select * \n",
    "from   train_master_all\n",
    "where  store_nbr = 20\n",
    "and    family2 = 1\n",
    "and date8 >= '20170816'\n",
    "--and id in (3003066,3003074)\n",
    "limit 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select * \n",
    "from   results\n",
    "where  store_nbr = 20\n",
    "and family2 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"d:/lge/pycharm-projects/kaggle_store_sales/output\"\n",
    "pdf.to_csv(f'{path}/submission/submission_cat2007_20220425.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_master_all\n",
    "df = spark.sql('''\n",
    "with temp1 as (\n",
    "    select id,date,date10,date8,store_nbr,family2,family\n",
    "    from   train_master_all\n",
    "    where  date8 >= '20170816'\n",
    "    order by id, store_nbr, family2\n",
    ")\n",
    "select *\n",
    "from   temp1\n",
    "order by cast(store_nbr as int)\n",
    "''')\n",
    "df.show(5,False)\n",
    "# pdf = df.toPandas()\n",
    "# pdf.to_csv(f'{path}/holidays_events_mart01.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "select count(case when score < 0.20 then 1 else null end) as s10\n",
    "     , count(case when score between 0.21 and 0.40 then 1 else null end) as s30      \n",
    "     , count(case when score between 0.41 and 0.60 then 1 else null end) as s50       \n",
    "     , count(case when score between 0.61 and 0.80 then 1 else null end) as s70       \n",
    "     , count(case when score > 0.81 then 1 else null end) as s90       \n",
    "from (\n",
    "    select max(a.scenario_id) as scenario_id, a.store_nbr, a.family2, a.score \n",
    "    from   results a,\n",
    "          ( select store_nbr, family2, max(score) score\n",
    "            from results \n",
    "            group by store_nbr, family2\n",
    "          ) b\n",
    "    where a.store_nbr = b.store_nbr\n",
    "    and   a.family2 = b.family2\n",
    "    and   a.score = b.score\n",
    "    group by a.store_nbr, a.family2, a.score\n",
    "    order by a.store_nbr, a.family2, a.score\n",
    "    )\n",
    "''').show(2000,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select scenario_id, store_nbr, family2, score \n",
    "--select *\n",
    "from results \n",
    "where 1=1\n",
    "and scenario_id like'%m001%'\n",
    "--and store_nbr = '1' \n",
    "--and family2 = '2'\n",
    "order by substr(scenario_id,13,4), substr(scenario_id,1,4), substr(scenario_id,5,4)\n",
    "limit 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select *\n",
    "from results \n",
    "where 1=1\n",
    "--and scenario_id like'%m001%'\n",
    "and scenario_id = 'x002f005d003y001t002m001c001'\n",
    "and store_nbr = '1' \n",
    "and family2 = '1'\n",
    "limit 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "select scenario_id, count(*)\n",
    "from (\n",
    "    select max(a.scenario_id) as scenario_id, a.store_nbr, a.family2, a.score \n",
    "    from   results a,\n",
    "          ( select store_nbr, family2, max(score) score\n",
    "            from results \n",
    "            group by store_nbr, family2\n",
    "          ) b\n",
    "    where a.store_nbr = b.store_nbr\n",
    "    and   a.family2 = b.family2\n",
    "    and   a.score = b.score\n",
    "    group by a.store_nbr, a.family2, a.score\n",
    "    order by a.store_nbr, a.family2, a.score\n",
    "     )\n",
    "where score > 0.81     \n",
    "group by scenario_id      \n",
    "order by scenario_id      \n",
    "''').show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "\n",
    "select scenario_id\n",
    "     , count(*) cnt\n",
    "     , count(case when score < 0.20 then 1 else null end) as s10\n",
    "     , count(case when score between 0.21 and 0.40 then 1 else null end) as s30      \n",
    "     , count(case when score between 0.41 and 0.60 then 1 else null end) as s50       \n",
    "     , count(case when score between 0.61 and 0.80 then 1 else null end) as s70       \n",
    "     , count(case when score > 0.81 then 1 else null end) as s90       \n",
    "     , max(scenario_desc) as scenario_desc\n",
    "     , max(feature_col ) as feature_col\n",
    "     , max(feature_sdt8) as feature_sdt8\n",
    "     , max(feature_edt8) as feature_edt8\n",
    "     , max(predict_col ) as predict_col\n",
    "     , max(predict_sdt8) as predict_sdt8\n",
    "     , max(predict_edt8) as predict_edt8\n",
    "     , round(sum(fit_tm_sec)/60)  as fit_tm_min\n",
    "from results                                                                                                                                                                     \n",
    "group by scenario_id\n",
    "order by scenario_id\n",
    "limit 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "select auto_ml_model, count(*)\n",
    "from results\n",
    "where scenario_id like '%m004'\n",
    "group by auto_ml_model\n",
    "order by auto_ml_model\n",
    "''').show(2,False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
